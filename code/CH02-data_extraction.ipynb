{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HPTxqCFkO1kI"
      },
      "outputs": [],
      "source": [
        "def process_bnc_tagged(filename=\"/content/bnc_tagged.txt\"):\n",
        "  \"\"\"'bnc_tagged.txt' 파일을 읽고 처리한 후, 결과를 반환합니다.\n",
        "\n",
        "  입력 데이터 형식:\n",
        "  - 읽고 처리할 코퍼스는 의존구조 파싱(dependency parsing)이 된 파일\n",
        "  - 각 문장은 '<s id=숫자>'로 시작하고 '</s>'로 끝남.\n",
        "  - 문장의 각 줄은 하나의 토큰을 나타나며, 탭으로 구분된 여러 정보를 포함함 (7열 탭 구분 데이터): 단어 형태, 품사(UPOS), 기본형(lemma), 단어 ID, 핵어 ID(head ID), 핵어(head word), 의존관계(deprel)\n",
        "\n",
        "  함수의 작동:\n",
        "  - 각 문장에서 탭으로 구분된 7개의 값으로 이루어진 단어 정보들을 모아서 리스트로 만든다.\n",
        "  - 문장 ID 와 단어 정보 리스트를 튜플로 묶어서 결과 리스트에 추가한다. 단어 정보가 7개의 값을 갖지 않으면 그 단어는 건너뛴다.\n",
        "\n",
        "  반환 (출력 데이터): (문장 ID, 토큰 리스트) 형태의 튜플을 저장한 리스트\n",
        "  \"\"\"\n",
        "  results = []\n",
        "  with open(filename, 'r', encoding='utf-8') as f:\n",
        "    current_sentence_id = None\n",
        "    current_sentence_tokens = []\n",
        "    for line in f:\n",
        "      line = line.strip()\n",
        "      if line.startswith('<s id='):\n",
        "        # 새 문장 시작\n",
        "        if current_sentence_id is not None:\n",
        "          results.append((current_sentence_id, current_sentence_tokens))\n",
        "        current_sentence_id = line[5:-1]  # '<s id=숫자>'에서 숫자만 추출\n",
        "        current_sentence_tokens = []\n",
        "      elif line == '</s>':\n",
        "        # 문장 끝\n",
        "        results.append((current_sentence_id, current_sentence_tokens))\n",
        "        current_sentence_id = None\n",
        "        current_sentence_tokens = []\n",
        "      else:\n",
        "        # 단어 정보 처리\n",
        "        token_info = line.split('\\t')\n",
        "        if len(token_info) == 7:\n",
        "          current_sentence_tokens.append(token_info)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_verb_dictionary(filename=\"/content/verb_list.txt\"):\n",
        "  \"\"\"'verb_list.txt' 파일을 읽고, 동사 딕셔너리를 생성하여 반환합니다.\n",
        "\n",
        "  입력 데이터 형식:\n",
        "  - 파일의 각 줄에는 코퍼스로부터 용례를 추출하려는 동사의 기본형(lemma)이 적혀 있음.\n",
        "\n",
        "  함수의 작동:\n",
        "  - 파일의 각 줄에서 읽어온 문자열(동사 기본형)을 key로 하고, 빈 리스트 '[]'를 value로 해서 딕셔너리에 추가한다.\n",
        "\n",
        "  반환 (출력 데이터):\n",
        "  - 동사를 key로 하고 빈 리스트를 value로 하는 딕셔너리\n",
        "  \"\"\"\n",
        "  verb_list = {}\n",
        "  with open(filename, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "      verb = line.strip()  # 각 줄에서 동사 기본형 추출\n",
        "      verb_list[verb] = []  # 동사를 key로, 빈 리스트를 value로 딕셔너리에 추가\n",
        "  return verb_list"
      ],
      "metadata": {
        "id": "GxfWR3z3RKnB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def extract_verb_examples(corpus_data, verb_dictionary):\n",
        "  \"\"\"코퍼스 데이터에서 본동사 용례를 추출하여 동사 딕셔너리에 추가합니다.\n",
        "\n",
        "  Args:\n",
        "    corpus_data: (문장 ID, 토큰 리스트) 형태의 튜플을 담은 리스트\n",
        "    verb_dictionary: 동사를 key로 하고, 빈 리스트를 value로 갖는 defaultdict(list) 형태\n",
        "\n",
        "  Returns:\n",
        "    동사 용례 딕셔너리\n",
        "  \"\"\"\n",
        "\n",
        "  for sentence_id, tokens in corpus_data:\n",
        "    for token in tokens:\n",
        "      lemma = token[2]  # 기본형\n",
        "      pos = token[1]  # 품사\n",
        "      deprel = token[6]  # 의존관계\n",
        "\n",
        "      if lemma in verb_dictionary and pos == \"VERB\" and deprel == \"root\":\n",
        "        sentence_text = \" \".join([t[0] for t in tokens])  # 문장 단어 연결\n",
        "        verb_dictionary[lemma].append((sentence_id, sentence_text))  # 용례 추가\n",
        "\n",
        "  return verb_dictionary"
      ],
      "metadata": {
        "id": "C3jMg7PHRWqH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def extract_passive_verb_examples(corpus_data, verb_dictionary):\n",
        "  \"\"\"코퍼스 데이터에서 수동태 구문에 사용된 동사 용례를 추출하여 동사 딕셔너리에 추가합니다.\n",
        "\n",
        "  Args:\n",
        "    corpus_data: (문장 ID, 토큰 리스트) 형태의 튜플을 담은 리스트\n",
        "    verb_dictionary: 동사를 key로 하고, 빈 리스트를 value로 갖는 defaultdict(list) 형태\n",
        "\n",
        "  Returns:\n",
        "    동사 용례 딕셔너리\n",
        "  \"\"\"\n",
        "  passive_verb_examples = defaultdict(list)\n",
        "\n",
        "  for sentence_id, tokens in corpus_data:\n",
        "    for i, token in enumerate(tokens):\n",
        "      lemma = token[2]  # 기본형\n",
        "      pos = token[1]  # 품사\n",
        "      deprel = token[6]  # 의존관계\n",
        "      head_id = token[4]  # head ID\n",
        "\n",
        "      # 조건 1: 기본형이 동사 딕셔너리 key에 포함, 품사가 VERB, 의존관계가 root\n",
        "      if lemma in verb_dictionary and pos == \"VERB\" and deprel == \"root\":\n",
        "        # 조건 4: aux:pass 의존관계를 갖는 의존소 확인\n",
        "        has_aux_pass = any(t[6] == \"aux:pass\" and t[4] == str(i + 1) for t in tokens)\n",
        "\n",
        "        # 조건 5: compound:prt 의존관계를 갖는 의존소 확인 (없어야 함)\n",
        "        has_compound_prt = any(t[6] == \"compound:prt\" and t[4] == str(i + 1) for t in tokens)\n",
        "\n",
        "        # 모든 조건을 만족하는 경우\n",
        "        if has_aux_pass and not has_compound_prt:\n",
        "          sentence_text = \" \".join([t[0] for t in tokens])  # 문장 단어 연결\n",
        "          passive_verb_examples[lemma].append((sentence_id, sentence_text))  # 용례 추가\n",
        "\n",
        "  return passive_verb_examples"
      ],
      "metadata": {
        "id": "RgpqgduPSEtx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def extract_transitive_verb_examples(corpus_data, verb_dictionary):\n",
        "  \"\"\"코퍼스 데이터에서 타동사 용례를 추출하여 동사 딕셔너리에 추가합니다.\n",
        "\n",
        "  Args:\n",
        "    corpus_data: (문장 ID, 토큰 리스트) 형태의 튜플을 담은 리스트\n",
        "    verb_dictionary: 동사를 key로 하고, 빈 리스트를 value로 갖는 defaultdict(list) 형태\n",
        "\n",
        "  Returns:\n",
        "    동사 용례 딕셔너리\n",
        "  \"\"\"\n",
        "  transitive_verb_examples = defaultdict(list)\n",
        "\n",
        "  for sentence_id, tokens in corpus_data:\n",
        "    for i, token in enumerate(tokens):\n",
        "      lemma = token[2]  # 기본형\n",
        "      pos = token[1]  # 품사\n",
        "      deprel = token[6]  # 의존관계\n",
        "      #head_id = token[4]  # head ID\n",
        "\n",
        "      # 조건 1: 기본형이 동사 딕셔너리 key에 포함, 품사가 VERB, 의존관계가 root\n",
        "      if lemma in verb_dictionary and pos == \"VERB\" and deprel == \"root\":\n",
        "        # 조건 4: obj 의존관계를 갖는 의존소 확인\n",
        "        has_obj = any(t[6] == \"obj\" and t[4] == str(i + 1) for t in tokens)\n",
        "\n",
        "        # 조건 5: aux:pass 또는 compound:prt 의존관계를 갖는 의존소 확인 (없어야 함)\n",
        "        has_aux_pass = any(t[6] == \"aux:pass\" and t[4] == str(i + 1) for t in tokens)\n",
        "        has_compound_prt = any(t[6] == \"compound:prt\" and t[4] == str(i + 1) for t in tokens)\n",
        "\n",
        "        # 모든 조건을 만족하는 경우\n",
        "        if has_obj and not has_aux_pass and not has_compound_prt:\n",
        "          sentence_text = \" \".join([t[0] for t in tokens])  # 문장 단어 연결\n",
        "          transitive_verb_examples[lemma].append((sentence_id, sentence_text))  # 용례 추가\n",
        "\n",
        "  return transitive_verb_examples"
      ],
      "metadata": {
        "id": "nvSc2ytbSl2D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def extract_intransitive_verb_examples(corpus_data, verb_dictionary):\n",
        "  \"\"\"코퍼스 데이터에서 자동사 용례를 추출하여 동사 딕셔너리에 추가합니다.\n",
        "\n",
        "  Args:\n",
        "    corpus_data: (문장 ID, 토큰 리스트) 형태의 튜플을 담은 리스트\n",
        "    verb_dictionary: 동사를 key로 하고, 빈 리스트를 value로 갖는 defaultdict(list) 형태\n",
        "\n",
        "  Returns:\n",
        "    동사 용례 딕셔너리\n",
        "  \"\"\"\n",
        "  intransitive_verb_examples = defaultdict(list)\n",
        "\n",
        "  for sentence_id, tokens in corpus_data:\n",
        "    for i, token in enumerate(tokens):\n",
        "      lemma = token[2]  # 기본형\n",
        "      pos = token[1]  # 품사\n",
        "      deprel = token[6]  # 의존관계\n",
        "\n",
        "      # 조건 1: 기본형이 동사 딕셔너리 key에 포함, 품사가 VERB, 의존관계가 root\n",
        "      if lemma in verb_dictionary and pos == \"VERB\" and deprel == \"root\":\n",
        "        # 조건 4: obj, aux:pass, compound:prt 의존관계를 갖는 의존소 확인 (없어야 함)\n",
        "        has_obj = any(t[6] == \"obj\" and t[4] == str(i + 1) for t in tokens)\n",
        "        has_aux_pass = any(t[6] == \"aux:pass\" and t[4] == str(i + 1) for t in tokens)\n",
        "        has_compound_prt = any(t[6] == \"compound:prt\" and t[4] == str(i + 1) for t in tokens)\n",
        "\n",
        "        # 모든 조건을 만족하는 경우 (obj, aux:pass, compound:prt가 없는 경우)\n",
        "        if not has_obj and not has_aux_pass and not has_compound_prt:\n",
        "          sentence_text = \" \".join([t[0] for t in tokens])  # 문장 단어 연결\n",
        "          intransitive_verb_examples[lemma].append((sentence_id, sentence_text))  # 용례 추가\n",
        "\n",
        "  return intransitive_verb_examples"
      ],
      "metadata": {
        "id": "FVo36qSDS9O5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_verb_examples(verb_examples_dict, folder, header, type):\n",
        "  \"\"\"동사 용례 딕셔너리를 두 개의 텍스트 파일로 저장합니다.\n",
        "\n",
        "  Args:\n",
        "    verb_examples_dict: 동사를 key로, 용례 리스트((문장 ID, 문장 문자열) 튜플)를 value로 갖는 딕셔너리\n",
        "    folder: 출력 파일을 저장할 폴더 이름\n",
        "    header: 출력 파일 이름 앞부분에 사용할 문자열\n",
        "    type: 용례 유형을 나타내는 문자열 (all, passive, transitive 등)\n",
        "  \"\"\"\n",
        "  import os\n",
        "\n",
        "  # 폴더 생성\n",
        "  os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "  # 파일 이름 생성\n",
        "  count_filename = os.path.join(folder, f\"{header}_word_{type}_count.txt\")\n",
        "  examples_filename = os.path.join(folder, f\"{header}_word_{type}.txt\")\n",
        "\n",
        "  # 빈도수 파일 저장\n",
        "  with open(count_filename, 'w', encoding='utf-8') as count_file:\n",
        "    for verb, examples in verb_examples_dict.items():\n",
        "      # 중복 제거\n",
        "      unique_examples = list(set(examples))\n",
        "      count = len(unique_examples)\n",
        "      if count > 0:\n",
        "        first_example = unique_examples[0][1]  # 첫 번째 용례 문장\n",
        "        count_file.write(f\"{verb}\\t{count}\\t{first_example}\\n\")\n",
        "\n",
        "  # 용례 파일 저장\n",
        "  with open(examples_filename, 'w', encoding='utf-8') as examples_file:\n",
        "    for verb, examples in verb_examples_dict.items():\n",
        "      for sentence_id, sentence_text in examples:\n",
        "        examples_file.write(f\"{verb}\\t{sentence_id}\\t{sentence_text}\\n\")"
      ],
      "metadata": {
        "id": "6qE0HdT3TFNg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "import os # import os module for file and folder operations\n",
        "\n",
        "\n",
        "# 이전 단계에서 정의한 함수들 가져오기\n",
        "# process_bnc_tagged, create_verb_dictionary, extract_verb_examples,\n",
        "# extract_passive_verb_examples, extract_transitive_verb_examples,\n",
        "# extract_intransitive_verb_examples, save_verb_examples\n",
        "\n",
        "# explicitly import required functions from other modules/cells\n",
        "from IPython.utils import io # Added to handle potential io errors\n",
        "\n",
        "# The with statement was incorrectly placed and is not needed in this case\n",
        "# with io.capture_output() as captured: #Added to handle potential io errors\n",
        "\n",
        "\n",
        "#import extract_intransitive_verb_examples from the relevant cell/module if they are defined there.\n",
        "#from . import extract_intransitive_verb_examples  # example assuming the function is in the same package\n",
        "#or\n",
        "#import notebook_name.module_name as module_containing_function\n",
        "#extract_intransitive_verb_examples = module_containing_function.extract_intransitive_verb_examples\n",
        "\n",
        "# if they are defined in separate notebook cells (assuming you are using notebooks)\n",
        "# you can call them directly, you don't need a separate import\n",
        "# but you need to make sure those cells have been run before this cell is executed.\n",
        "\n",
        "\n",
        "# assuming the other functions are also in separate cells and those cells are run:\n",
        "# from . import process_bnc_tagged\n",
        "# from . import create_verb_dictionary\n",
        "# from . import extract_verb_examples\n",
        "# from . import extract_passive_verb_examples\n",
        "# from . import extract_transitive_verb_examples\n",
        "# from . import save_verb_examples\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    'bnc_tagged.txt'와 'verb_list.txt'를 입력으로 받아 동사 용례를 추출하고 저장하는 메인 함수입니다.\n",
        "    \"\"\"\n",
        "    corpus_file = \"/content/bnc_tagged.txt\"\n",
        "    verb_list_file = \"/content/verb_list.txt\"\n",
        "    output_folder = \"output\"\n",
        "\n",
        "    # 1. 코퍼스 전처리\n",
        "    corpus_data = process_bnc_tagged(filename=corpus_file)\n",
        "\n",
        "    # 2. 동사 목록 전처리\n",
        "    verb_dictionary = create_verb_dictionary(filename=verb_list_file)\n",
        "\n",
        "    # 3. 용례 추출\n",
        "    all_verb_examples = extract_verb_examples(corpus_data, verb_dictionary.copy())\n",
        "    passive_verb_examples = extract_passive_verb_examples(corpus_data, verb_dictionary.copy())\n",
        "    transitive_verb_examples = extract_transitive_verb_examples(corpus_data, verb_dictionary.copy())\n",
        "    intransitive_verb_examples = extract_intransitive_verb_examples(corpus_data, verb_dictionary.copy())\n",
        "\n",
        "    # 4. 결과 처리\n",
        "    save_verb_examples(all_verb_examples, output_folder, \"bnc\", \"all\")\n",
        "    save_verb_examples(passive_verb_examples, output_folder, \"bnc\", \"passive\")\n",
        "    save_verb_examples(transitive_verb_examples, output_folder, \"bnc\", \"transitive\")\n",
        "    save_verb_examples(intransitive_verb_examples, output_folder, \"bnc\", \"intransitive\")\n",
        "\n",
        "    # 5. 코퍼스 전처리 결과 저장\n",
        "    with open(os.path.join(output_folder, \"bnc_corpus_data.txt\"), 'w', encoding='utf-8') as f:\n",
        "        for sentence_id, tokens in corpus_data:\n",
        "            f.write(f\"{sentence_id}\\t{' '.join([t[0] for t in tokens])}\\n\")  # 문장 ID와 문장 텍스트를 탭으로 구분하여 저장\n",
        "\n",
        "    # 6. 동사 목록 전처리 결과 저장\n",
        "    with open(os.path.join(output_folder, \"verb_dictionary.txt\"), 'w', encoding='utf-8') as f:\n",
        "        json.dump(verb_dictionary, f, indent=2)  # JSON 형식으로 저장\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "i0SmD6o7TcDB"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}